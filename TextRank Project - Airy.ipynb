{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "What:\n",
    "TextRank uses an extractive approach and is an unsupervised graph-based text summarization technique. \n",
    "\n",
    "How: \n",
    "The basic idea implemented by a graph-based ranking model is that of voting or recommendation.\n",
    "When one vertex links to another one, it is basically casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher the importance of that vertex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk menjalankan project TextRank ini, diperlukan untuk :\n",
    "1. men-download library stopwords Bahasa Indonesia: Python Sastrawi.\n",
    "    Cara download library:\n",
    "    1. Open terminal\n",
    "    2. write on command line : \"pip install Sastrawi\"\n",
    "2. men-download Python ROUGE\n",
    "    Cara download library:\n",
    "    1. Open terminal\n",
    "    2. write on command line : \"pip install py-rouge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "import urllib\n",
    "import os, os.path\n",
    "#import modular regular expression\n",
    "import re\n",
    "import string\n",
    "# import StopWordRemoverFactory class\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open txt file and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing methods used:\n",
    "1. decode to UTF-8 -> standardisation\n",
    "2. change all to lower case -> consistency purposes especially during stopwords removal process\n",
    "3. remove all numbers -> avoid redundancy\n",
    "4. remove all punctuation -> avoid redundancy\n",
    "5. stopwords (using Python Sastrawi) -> avoid redundancy\n",
    "6. Stemmer (using Python Sastrawi) -> easy filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kotor debu tdk henti bersin awal masuk kamar kesan kamar kumuh wifi rusak lgsg check out\n",
      "oke cuma air wastafel warna keruh mohon baik\n",
      "kamar semut kamar mandi masalah buang air mampetsehingga air luber luar kamar\n",
      "kamar mandi bau air bau\n",
      "kamar bersih nyaman perlu perhati dan trus kroscheck trutama slot card pintu kamar batrai nya habis apa msih full kasi ada tamu lelah habis jalan jauh tunggu luar karena slotcard pintu kamar di perbaikin ulang tks smoga manfaat\n",
      "tak sesuai espektasi kamar sempit pintu kamar mandi rusak cek in tunggu terlalu lama malam suara berisik dr atap kamar nyaman\n",
      "booking traveloka room sedia kembali uang nya lamaaa\n",
      "pesan kamar double bad dapat kamar twins bad\n",
      "buruk kasur bekas sperma seprai jg air bau karat kasur per jg rusak sabun habis pasword wifi ga tau\n",
      "baik puas masih kurang waktu boking airy rooms pilih bad tp saya cek in kasih bad mohon di tingkat terimah kasih airy rooma\n",
      "kamar mandi kurang bersih kurang lengkap faslitasnya kamar mandi yang lumayan\n",
      "lokasi susah temu dari luar lihat nama proses reservasi sedikit lama roomnya nyaman bersih\n",
      "sarung bantal bau\n",
      "kamar luas bersih makan kurang\n",
      "lumayan lah sangat jangkau perlu yg benah soal bersih kasur kemarin dapat kasur jorok banyak helai rambur seperti bersih ganti seprei bekas unjung belum\n",
      "suasana kmr yg prlu direfresh dn agk sempit ukrn kmr\n",
      "layan cukup baik tempat kurang strategis\n",
      "kesan pertama dapat cukup bingung karena hotel nama airy nyata ada dalam bhotel tolong saran tambah informasi sehingga orang yang mes kali langsung eta\n",
      "kamar mandi bau lubang salur air mampat penuh rambut harus yang bersih kurang meja kecil samping tempat tidur kamar kecil sangat nyaman\n",
      "segi fasilitas nya sekarang kurang kamar mandi nya showernya ada ikat layan sedikit lebih d tingkat lagi\n",
      "kamar nyaman acnya berbunyilantai kamar kurang bersih makan unuk sarap enak terima kasih airy\n",
      "lumayan bersih ramah cuma kamar kedap suara betul kamar dekat dng resepsionis klo resepsionis bicara dengar kamar\n",
      "pas hujan air masuk kamar hotel\n",
      "bau rokok\n",
      "sangat emejing inap airy sutoyo hotel dewarnaudah murah tempat nyaman buat istirahatapalagi breakfastnya nikmat ga asal harga murah kualitas murahantp yg perlu baik cuman bersih kamar kurang bersih klo ngepeloverall rekomen bgt\n",
      "kamar spt kos kos an ada handuk ada hot water parkir mobil sempit\n",
      "sprei kasur handuk kurangp bersih tetapiklo lebih bersih tambah oke\n",
      "ada layan tonton televisi lain muas oke\n",
      "cukup oke pelayanannyacuma handuk kurang bersih terima kasih\n",
      "sangat muas sprei handuk wangi kamar cukup bersih nyaman ac air panas fungsi baik salur tv sedikit masalah cukup muas alam dua di airy gunung sahari terimakasih\n",
      "kamar sangat kecewa pintu lemari rusak toilet kamar mandi keran tdk fungsi dgn baik\n",
      "layan cek in lama\n",
      "baik selalu kembalicuma baik wifi hot waternya utk kamar tentu\n",
      "cukup bagussesuai tarif air panas showertidak keluar pagi haritrus makan sarap pagi kurang enak\n",
      "kasur selimut kotor\n",
      "puas inap d airy room lovina inncuma agk sayng wktu cek inkmarny bru d setting k airy\n",
      "sangat baik semua nyahanya tdk air panas utk mandisdg baik\n",
      "ac nya parah berisik bersurara bikin gak tidur sama sekali lokasi nya susah minta ampun temu\n",
      "awal wifi bsgus hari wifi nya mati keseluruhanya ok\n",
      "hotel nyaman bersih hanya jauh masuk\n",
      "sesuai foto kecewa\n",
      "bangun bersih tempat parkir cukup bagus sayang sedia break fast\n",
      "inap cukup rensentatif sayang air panas maksimal\n",
      "luar ekspektasi gambar lihat bagus asli sama sekali bayang belum\n",
      "saja ruang terlalu sempit\n",
      "the tv should be upgrade and also the toiletaries\n",
      "masuk tegal malam familiar kota tegl bolak balek lewat jalan sudirman koq tidak ada hotel airy telpon terjnyata arah hotel alexander pake naek tangga kami usia an sepi sekali te\n",
      "kamar bau selimut handuk kusam staffnya gak ramah kamar tdk sesuai ekspektasi sempit pengap breakfast kayak makan kucing jd cari sarap luar harga segitu gak worth it\n",
      "better find another hotel unless you just want stay for a night and no need comfort room it was so old and not so clean the bed and toilet lucky airy give a little bag and refreshments snack\n",
      "kecewa harus hari sana hari langsung cabut\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jordhy97/final_project/master/data/reviews/reviews.txt\"\n",
    "file = urllib.request.urlopen(url)\n",
    "\n",
    "#defining function getLines() = to get only the first 50 lines of review\n",
    "def getLines(file):\n",
    "    res = []\n",
    "    i = 0\n",
    "\n",
    "    while i < 50:\n",
    "        res.append(str(file.readline(), \"utf-8\"))\n",
    "        i+=1\n",
    "\n",
    "    return res\n",
    "\n",
    "file_lines = getLines(file)\n",
    "\n",
    "stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "res_preprocessed_line = []\n",
    "\n",
    "for line in file_lines:\n",
    "#     preprocessed_line = line.decode(\"utf-8\")\n",
    "    preprocessed_line = line.lower()\n",
    "    preprocessed_line = re.sub(r\"\\d+\", \"\", preprocessed_line)\n",
    "    preprocessed_line = preprocessed_line.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    preprocessed_line = stopword.remove(preprocessed_line)\n",
    "    preprocessed_line = stemmer.stem(preprocessed_line)\n",
    "    print(preprocessed_line)\n",
    "    res_preprocessed_line.append(preprocessed_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first build a graph associated with the text, where the graph vertices are representative for the units to be ranked. The goal is to rank entire sentences, therefore, a vertex is added to the graph for each sentence in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some conditions a good similarity measure has to obey:\n",
    "\n",
    "1. 0 <= similarity(A, B) <= 1\n",
    "2. similarity(A, A) = 1\n",
    "3. similarity(A, B) =/= 1 if A =/= B\n",
    "\n",
    "Note: A and B are different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used measure in Natural Language Processing is the Cosine Similarity. The Cosine Similarity computes the cosine of the angle between 2 vectors. If the vectors are identical, the cosine is 1.0. If the vectors are orthogonal, the cosine is 0.0. This means the cosine similarity is a measure we can use. NLTK implements cosine_distance, which is 1 - cosine_similarity. The concept of distance is opposite to similarity. Two identical vectors are located at 0 distance and are 100% similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Similarity using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: \n",
      "50\n",
      "\n",
      "[[0.         0.02184822 0.02021487 ... 0.02207463 0.02021411 0.02016505]\n",
      " [0.02178269 0.         0.02098525 ... 0.02140803 0.01968865 0.02081242]\n",
      " [0.02021233 0.02104574 0.         ... 0.02177392 0.01723683 0.02091857]\n",
      " ...\n",
      " [0.02138501 0.02080162 0.02109634 ... 0.         0.01845196 0.02060326]\n",
      " [0.02208241 0.02157309 0.01883232 ... 0.02080743 0.         0.0198931 ]\n",
      " [0.02044759 0.02116751 0.02121433 ... 0.02156566 0.01846517 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def build_similarity_matrix(res_preprocessed_line, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(res_preprocessed_line), len(res_preprocessed_line)))\n",
    " \n",
    "    for idx1 in range(len(res_preprocessed_line)):\n",
    "        for idx2 in range(len(res_preprocessed_line)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(res_preprocessed_line[idx1], res_preprocessed_line[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " \n",
    "print(\"Total number of reviews: \")\n",
    "print(len(res_preprocessed_line))\n",
    "print()\n",
    "\n",
    "stop_words = \"\"\n",
    "S = build_similarity_matrix(res_preprocessed_line, stop_words)    \n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining pagerank function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs(new_P - P).sum()\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02044427 0.02049617 0.02043572 0.01931903 0.02115599 0.02101363\n",
      " 0.02007606 0.0199717  0.02018566 0.02037842 0.020355   0.02059009\n",
      " 0.01827864 0.02031692 0.02095459 0.01957454 0.02031946 0.02071954\n",
      " 0.02098467 0.02056576 0.02054052 0.0209514  0.0205534  0.01409787\n",
      " 0.02110922 0.02047377 0.01975169 0.01956799 0.02046342 0.02089271\n",
      " 0.02086726 0.0186872  0.02035053 0.02069653 0.01704985 0.02015594\n",
      " 0.02036591 0.0211268  0.02000119 0.01966695 0.0166519  0.02058684\n",
      " 0.02023277 0.02031293 0.0196461  0.01827864 0.02062807 0.02100153\n",
      " 0.01896214 0.02019307]\n",
      "[4, 37, 24, 5, 47, 18, 14, 21, 29, 30, 17, 33, 46, 11, 41, 19, 22, 20, 1, 25, 28, 0, 2, 9, 36, 10, 32, 16, 13, 43, 42, 49, 8, 35, 6, 38, 7, 26, 39, 44, 15, 27, 3, 48, 31, 12, 45, 34, 40, 23]\n",
      "[4, 5, 24, 37, 47]\n",
      "kamar bersih nyaman perlu perhati dan trus kroscheck trutama slot card pintu kamar batrai nya habis apa msih full kasi ada tamu lelah habis jalan jauh tunggu luar karena slotcard pintu kamar di perbaikin ulang tks smoga manfaat\n",
      "tak sesuai espektasi kamar sempit pintu kamar mandi rusak cek in tunggu terlalu lama malam suara berisik dr atap kamar nyaman\n",
      "sangat emejing inap airy sutoyo hotel dewarnaudah murah tempat nyaman buat istirahatapalagi breakfastnya nikmat ga asal harga murah kualitas murahantp yg perlu baik cuman bersih kamar kurang bersih klo ngepeloverall rekomen bgt\n",
      "ac nya parah berisik bersurara bikin gak tidur sama sekali lokasi nya susah minta ampun temu\n",
      "kamar bau selimut handuk kusam staffnya gak ramah kamar tdk sesuai ekspektasi sempit pengap breakfast kayak makan kucing jd cari sarap luar harga segitu gak worth it\n"
     ]
    }
   ],
   "source": [
    "sentence_ranks = pagerank(S)\n",
    " \n",
    "print(sentence_ranks)\n",
    " \n",
    "# Get the sentences ordered by rank\n",
    "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "print(ranked_sentence_indexes)\n",
    " \n",
    "# Suppose we want the 5 most import sentences\n",
    "SUMMARY_SIZE = 5\n",
    "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
    "print(SELECTED_SENTENCES)\n",
    " \n",
    "# Fetch the most important sentences\n",
    "summary = itemgetter(*SELECTED_SENTENCES)(res_preprocessed_line)\n",
    " \n",
    "# Print the actual summary\n",
    "for line in summary:\n",
    "    print(''.join(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the top 5 summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. kamar bersih nyaman perlu perhati dan trus kroscheck trutama slot card pintu kamar batrai nya habis apa msih full kasi ada tamu lelah habis jalan jauh tunggu luar karena slotcard pintu kamar di perbaikin ulang tks smoga manfaat\n",
      "2. tak sesuai espektasi kamar sempit pintu kamar mandi rusak cek in tunggu terlalu lama malam suara berisik dr atap kamar nyaman\n",
      "3. sangat emejing inap airy sutoyo hotel dewarnaudah murah tempat nyaman buat istirahatapalagi breakfastnya nikmat ga asal harga murah kualitas murahantp yg perlu baik cuman bersih kamar kurang bersih klo ngepeloverall rekomen bgt\n",
      "4. ac nya parah berisik bersurara bikin gak tidur sama sekali lokasi nya susah minta ampun temu\n",
      "5. kamar bau selimut handuk kusam staffnya gak ramah kamar tdk sesuai ekspektasi sempit pengap breakfast kayak makan kucing jd cari sarap luar harga segitu gak worth it\n"
     ]
    }
   ],
   "source": [
    "def textrank(res, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    res_preprocessed_line = a list of airy review sentences \n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(res_preprocessed_line, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(res_preprocessed_line)\n",
    "    return summary\n",
    "\n",
    "candidates = []\n",
    "\n",
    "for idx, line in enumerate(textrank(res_preprocessed_line, stopwords=StopWordRemoverFactory().create_stop_word_remover())):\n",
    "    print(\"%s. %s\" % ((idx + 1), ''.join(line)))\n",
    "    candidates.append(''.join(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the purpose of this project, we will be using ROUGE and BLEU metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are libraries for BLEU metrics in nltk. Thus, in this project, we will use them.\n",
    "\n",
    "Difference between corpus_bleu and sentence_bleu:\n",
    "    1. corpus_bleu: to evaluate some sentences in a paragraph or document\n",
    "    2. sentence_bleu: to evaluate one sentence against some reference sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Metrics Summary Evaluation\n",
      "\n",
      "kamar bersih nyaman perlu perhati dan trus kroscheck trutama slot card pintu kamar batrai nya habis apa msih full kasi ada tamu lelah habis jalan jauh tunggu luar karena slotcard pintu kamar di perbaikin ulang tks smoga manfaat\n",
      "score for this summary: : 0.06138813788678699\n",
      "\n",
      "tak sesuai espektasi kamar sempit pintu kamar mandi rusak cek in tunggu terlalu lama malam suara berisik dr atap kamar nyaman\n",
      "score for this summary: : 0.21271927853112058\n",
      "\n",
      "sangat emejing inap airy sutoyo hotel dewarnaudah murah tempat nyaman buat istirahatapalagi breakfastnya nikmat ga asal harga murah kualitas murahantp yg perlu baik cuman bersih kamar kurang bersih klo ngepeloverall rekomen bgt\n",
      "score for this summary: : 0.057512116804495854\n",
      "\n",
      "ac nya parah berisik bersurara bikin gak tidur sama sekali lokasi nya susah minta ampun temu\n",
      "score for this summary: : 0.08157811732948411\n",
      "\n",
      "kamar bau selimut handuk kusam staffnya gak ramah kamar tdk sesuai ekspektasi sempit pengap breakfast kayak makan kucing jd cari sarap luar harga segitu gak worth it\n",
      "score for this summary: : 0.18368560079061017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "references = [\"tak sesuai ekspektasi kamar pengap\"]\n",
    "\n",
    "idx = 0\n",
    "print(\"BLEU Metrics Summary Evaluation\")\n",
    "print()\n",
    "\n",
    "for candidate in candidates:\n",
    "    candidate.split()\n",
    "    print(candidate)\n",
    "    score = sentence_bleu(references, candidate)\n",
    "    print(\"%s: %s\" % (\"score for this summary: \", score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of ROUGE Metrics:\n",
    "1. ROUGE-N – measures unigram, bigram, trigram and higher order n-gram overlap. ROUGE-N may consist of ROUGE-1 and ROUGE-2, where it analyzes unigram and bigram overlap respectively.\n",
    "2. ROUGE-L –  measures longest matching sequence of words using LCS. An advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order. Since it automatically includes longest in-sequence common n-grams, you don’t need a predefined n-gram length.\n",
    "3. ROUGE-S – Is any pair of word in a sentence in order, allowing for arbitrary gaps. This can also be called skip-gram coocurrence. For example, skip-bigram measures the overlap of word pairs that can have a maximum of two gaps in between words. As an example, for the phrase “cat in the hat” the skip-bigrams would be “cat in, cat the, cat hat, in the, in hat, the hat”. \n",
    "\n",
    "\n",
    "Quoted from with some changes:\n",
    "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.XpQIZ2wzZuR\n",
    "\n",
    "##### Consideration for choosing ROUGE-1:\n",
    "We are developing summary with extractive summarization method with fairly verbose system and reference summaries, then it may make sense to use ROUGE-1 and ROUGE-L. For very concise summaries, ROUGE-1 alone may suffice especially if you are also applying stemming and stop word removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with Avg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-79a6706161d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Default F1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                            \u001b[0mweight_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                            stemming=True)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mreferences_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tak sesuai ekspektasi kamar pengap\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_n'"
     ]
    }
   ],
   "source": [
    "import rouge\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n",
    "    references_1 = \"tak sesuai ekspektasi kamar pengap\"\n",
    "    references_2 = \"ac nya parah berisik bersurara bikin gak tidur\"\n",
    "    references_3 = \"slotcard pintu kamar di perbaikin ulang\"\n",
    "    \n",
    "    all_hypothesis = [candidate[0], candidate[1], candidate[2], candidate[3], candidate[4]]\n",
    "    all_references = [references_1, references_2]\n",
    "\n",
    "    scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print(prepare_results(results['p'], results['r'], results['f']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Reference\n",
    "\n",
    "- Preprocessing techniques ideas are inspired by the links below: \n",
    "    1. https://www.academia.edu/35015140/Preprocessing_Techniques_for_Text_Mining\n",
    "    2. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "\n",
    "- Reference for data shortening:\n",
    "    1. https://stackoverflow.com/questions/10249673/python-read-lines-of-website-source-code-100-lines-at-a-time\n",
    "    \n",
    "- References for library for Indonesian Stopwords: \n",
    "    1. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "    2. https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\n",
    "    \n",
    "- Code for the modelling of the project is mainly referred from this website: \n",
    "    1. https://nlpforhackers.io/textrank-text-summarization/ \n",
    "    2. I do not take ownership of the code presented above\n",
    "    \n",
    "- ROUGE Metrics:\n",
    "    1. https://pypi.org/project/py-rouge/\n",
    "\n",
    "- BLEU Metrics:\n",
    "    1. https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reference (for reading references only)\n",
    "\n",
    "Below are links I have been referring to throughout the making of the project and want to revisit in the future:\n",
    "- TextRank algorithms: \n",
    "    1. https://iq.opengenus.org/textrank-for-text-summarization/\n",
    "    2. https://github.com/summanlp/textrank\n",
    "    3. https://github.com/davidadamojr/TextRank\n",
    "    4. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "    5. https://medium.com/@shivangisareen/text-summarisation-with-gensim-textrank-46bbb3401289\n",
    "    6. https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "- Text Summarization algorithm using word frequency: \n",
    "    1. https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "- Metrics (BLEU):\n",
    "    1. https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "    2. http://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "    3. https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213\n",
    "    4. http://www.nltk.org/api/nltk.translate.html\n",
    "    5. https://www.aclweb.org/anthology/P02-1040.pdf\n",
    "- Metrics (ROUGE):\n",
    "    1. https://github.com/pltrdy/rouge\n",
    "    2. https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.XpQIZ2wzZuR\n",
    "    3. https://www.aclweb.org/anthology/W04-1013.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
